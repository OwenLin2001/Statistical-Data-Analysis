---
title: "Stats 110 Homework 2"
author: "Owen Lin"
output: html_document
---

1. a. False
    b. False
    c. True
    d. False
    e. True
    f. False
<br>

2. a. Same, R is the correlation coefficient of X and Y, it measures how strong is the linear association between the two variables.
    b. Same, because R is the same, so R^2 is the same
    c. $\beta_1$ (slope) would be different (unless originally slope = 1)
    d. $\beta_0$ (y-intercept) would be different too, since X are Y switch position and the new y-intercept is the x-intercept before.
    e. Same, because R is the same and $T = \dfrac{R\sqrt{n-2}}{\sqrt{1-R^2}}$
    f. The residual for each each observation would be different because both variables changes as residual = predicted value - observed value
<br>

3. a. See Code Outputs

```{r, echo = FALSE, eval= FALSE}
setwd("D:\\Coding\\Data\\Stats110")
```

```{r, results = "hold"}
MidWestSales <- read.table("D:\\Coding\\Data\\Stats110\\MidwestSales.txt", fill = TRUE, header = FALSE) # nolint
names(MidWestSales) <- c("id","price","sqft","bed","bath","ac","garage","pool","year","quality","style","lot","hwy") # nolint
model <- lm(price ~ sqft, data = MidWestSales)
summary(model)$coef
print(paste("y = ", model$coefficient[1], "+", model$coefficient[2], "x"))
```

b. For every one square footage increase of the house, we expect the price of the house to increase by 158.95 dollar
c. H0: square footage has no linear relationship with price ($\beta_1 = 0$) <br>
    Ha: square footage has a linear relationship with price ($\beta_1 \neq 0$) <br>
    T-statistic: 32.605, which follows t-distribution with a degree of freedom of 520 <br>
    p-value = 8.284610e-128 < 0.05 <br>
    Thus, we reject the null and conclude that square footage has a significant linear relationship with price
d. H0: square footage has no **positive** linear relationship with price ($\beta_1 \leq 0$) <br>
    Ha: square footage has a significant **positive** linear relationship with price ($\beta_1 > 0$)<br>
    T-statistic: 32.605, which follows t-distribution with a degree of freedom of 520 <br>
    p-value = 8.284610e-128/2 = 4.142305e-128 < 0.05 <br>
    Thus, we reject the null and conclude that square footage has a significant **positive** linear relationship with price.

```{r, results= "hold"}
CI <- predict(model, list(sqft = 2000), interval = "c", level = 0.95)
PI <- predict(model, list(sqft = 2000), interval = "p", level = 0.95)
print(paste("The 95% confidence interval for the mean price when sqft=2000 is [", CI[2], ",", CI[3], "]")) #nolint
print(paste("The 95% prediction interval for the price when sqft=2000 is [", PI[2], ",", PI[3], "]")) # nolint
summary(MidWestSales$sqft)
```

e. We are 95% confident that the interval (229220.67, 243714.36) contains the **average** price when sqft = 2000 
f. We are 95% confident that the interval (80858.85, 392076.18) contains the price when sqft = 2000.
g. The interval in part f would be narrower if we decreased the confidence level to 90%, because 
    lower confidence means we try to predict more precisely.
h. No, it doesn't makes sense to predict the sale price of a house that is 8500 square feet, because the domain of our dataset is from 
    980 to 5032. There is no reason to believe that the linear trend continues to 8500 sqft houses.
i. The interval would cover all real number if the confidence level is increased to 100%, because there can always be outliers.
    It is unlikely to happen doesn't mean it will not happen. To be 100% confident, the interval needs to cover the entire domain.
j. The estimate of $\sigma_\epsilon$ is 79122.9. This is the square root of the variance in the observed price that is not explained
    by the variation in square footage of the house.

```{r}
summary(model)$sigma
```

k. No, because the bigger house can varied more in price. It can be a big but plain house with low price 
    or a big and fully decorated house with high price. On the other hand, the smaller house has less space
    for decoration => the price can't varies as much.
<br>

4. a. Yes, latitude has a linear association with the mortality because the p-value for $\beta_1$ is 3.31e-23 < 0.05
```{r, results = "hold"}
skincancer <- read.table("D:\\Coding\\Data\\Stats110\\skincancer.txt", fill = TRUE, header = TRUE) #nolint
head(skincancer)
model2 <- lm(Mort ~ Lat, data = skincancer)
summary(model2)
```

```{r, results= "hold"}
CI <- predict(model2, list(Lat = 40), interval = "c", level = 0.99)
PI <- predict(model2, list(Lat = 40), interval = "p", level = 0.99)
# print(confint(model2, level = 0.99)) CI for estimators
print(paste("The 99% confidence interval for the mean mortality rate when Lat=40 is [", CI[2], ",", CI[3], "]")) #nolint
print(paste("The 99% prediction interval for the mortality rate when Lat=40 is [", PI[2], ",", PI[3], "]")) # nolint
```

b. We are 99% confident that the interval (142.71, 157.45) contains the **average** mortality rate when latitude is 40.
c. We are 99% confident that the interval (98.24, 201.93) contains the mortality rate when latitude is 40.
d. The center of the confidence interval and prediction interval is the same, because the only difference between the confidence and 
    the prediction interval lies in standard error.
e. The width of the confidence interval is narrower compare to the prediction interval, because the standard error for the 
    prediction interval for the actual mortality rate has an extra MSE term (which is strictly positive)
<br>

5. a. Sample size is increased => prediction interval will be narrower
    b. If Xp gets closer to  Ì„X => prediction interval will be narrower
    c. If the variability of the response variable decreases => prediction interval will be narrower
    d. The average of the response is increased => prediction interval stay the same (but shift upward)
<br>

6. $$R^2 = \dfrac{SSR}{SSTO} = \dfrac{SSR}{(SSR+SSE)} = \dfrac{110}{110+40} = 0.73333$$
    73% of the variation in Y is explained by X
<br>

7. a. H0: Resting pulse rate has no linear relationship with smoking status ($\beta_1 = 0$) <br>
    Ha: Resting pulse rate has a linear relationship with smoking status ($\beta_1 \neq 0$) <br>
    T-statistic: 2.429, which follows t-distribution with a degree of freedom of 230 <br>
    p-value = 0.0159 < 0.05 <br>
    Thus, we reject the null and conclude that resting pulse rate has a linear relationship with smoking status

```{r, results = "hold"}
pulse <- read.table("D:\\Coding\\Data\\Stats110\\Pulse.txt", fill = TRUE, header = TRUE) #nolint
pulse$Smoker <- ifelse(pulse$Smoke == 1, "Yes", "No")
# head(pulse)
# ls(pulse)
model3 <- lm(Rest ~ Smoke, data = pulse)
summary(model3)
```

b. The t-statistic is the same as the test from part a (t = 2.429), and so is the p-value and the conclusion.

```{r, results = "hold"}
t.test(pulse$Rest[pulse$Smoker=="Yes"], pulse$Rest[pulse$Smoker=="No"], var.equal=TRUE) #nolint
```

c.  Population model: $y = \beta_0 + \beta_1* X1 + \beta_2* X2 + \epsilon$, Y: resting pulse rate, x1: weight, x2: smoking status
    estimated regression equation: $y = 78.247 - 0.067X1 + 6.043X2$

```{r, results = "hold"}
model4 <- lm(Rest ~ Wgt + Smoke, data = pulse)
round(coef(summary(model4)), 3)
```

d. Without an interaction term, we are assuming that the slope on weight with respect to resting pulse rate 
    is the same for smoker and non-smoker. In other words, the smoking status is only an additive parallel shift.
e. 
```{r, results = "hold"}
# ls(summary(model4))
summary(model4)
rsq <- summary(model4)$r.squared
sigma <- summary(model4)$sigma
print(paste("The coefficient of determination (multiple R^2) is", round(rsq, 3))) #nolint
print(paste("The estimate of stardard error on the error term is", round(sigma, 3))) #nolint
```

f. There are 229 + 3 = 232 observations
g. H0: Smoking status does not affect resting pulse rate ($\beta_2 = 0$) <br>
    Ha: Smoking status does affect resting pulse rate ($\beta_2 \neq 0$) <br>
    T-statistic: 2.975, which follows t-distribution with a degree of freedom of 229 <br>
    p-value = 0.00325 < 0.05 <br>
    Thus, we reject the null and conclude that Smoking status does affect resting pulse rate
h. We can't conclude that smoking causes lower resting pulse rate because the data did not come from not an controlled experiment.
i. For Non-smoker, there seems to be some negative association between weight and resting pulse rate. 
    For smokers, there are more noise in the data and we can't really tell if there's an association.
```{r, results = "hold"}
plot(pulse$Wgt[pulse$Smoker=="Yes"] , pulse$Rest[pulse$Smoker=="Yes"], xlab="Wgt", #nolint
       ylab ="Rest", main="Scatterplot of Wgt vs Rest", col="red", xlim=c(100,280), ylim=c(40,110)) #nolint
points(pulse$Wgt[pulse$Smoker=="No"] , pulse$Rest[pulse$Smoker=="No"], col = "blue", pch = 24) #nolint
legend(240,110,legend=c("Smoker", "Non-Smoker"),pch=c(1,24), col=c("red","blue")) #nolint
```

j.  Population Model: $y = \beta_0 + \beta_1* X1 + \beta_2* X2 + \beta_3* X1*X2 + \epsilon$ , Y: resting pulse rate, x1: weight, x2: smoking status, x3: interaction term
```{r, results = "hold"}
model5 <- lm(Rest ~ Wgt + Smoke + Wgt*Smoke, data = pulse)
round(coef(summary(model5)), 3)
```

k. Estimated regression equation: $y = 80.430 - 0.081X1 + 10.031X2 + 0.095X2*X3$
l. R square increase and standard deviation of the error term decreased as we add the interaction term.
```{r, results = "hold"}
rsq <- summary(model5)$r.squared
sigma <- summary(model5)$sigma
summary(model5)
ls(model5)
print(paste("The coefficient of determination (multiple R^2) is", round(rsq, 3))) #nolint
print(paste("The estimate of stardard error on the error term is", round(sigma, 3))) #nolint
```

m. H0: The effect of weight on resting pulse rate is the same for smokers and non-smokers ($\beta_3 = 0$) <br>
    Ha: The effect of weight on resting pulse rate differs for smokers and non-smokers ($\beta_3 \neq 0$) <br>
    T-statistic: 1.672, which follows t-distribution with a degree of freedom of 228 <br>
    p-value = 0.0959 > 0.05 <br>
    At 95 confident level, we fail to reject the null. Whether the effect of weight on resting pulse 
    rate differs for smokers and non-smokers is still inconclusive.
n. No, the value of R square is 0.07, which means only 7 percents of variation in resting pulse rate is explained by the weight.
o. We can infer that people who weight more exercise less, which can be a factor that decreases resting pulse rate.
p. H0: Adding weight as a variable doesn't improve R square <br>
    Ha: Adding weight as a variable does improve R square significantly
    